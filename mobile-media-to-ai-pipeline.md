## 手机媒体 → AI 内容自动化蓝图

### 场景概述
日常使用手机拍摄的图片 / 视频，需要自动上传到一个统一的存储位置，供大模型定期读取并生成配套文案或总结，最终通过邮件定时分发。

### 目标需求
1. 移动端拍摄的素材（图片、短视频）能在拍摄后最短时间内自动同步到云端。
2. 大模型能够访问该存储位置的新增文件，提取关键信息，生成文本内容或其他衍生资料。
3. 生成的内容与原始素材关联（例如含有文件名、上传时间等元数据）。
4. 最终产出通过既有的邮件流程（GitHub Actions/服务器）按日定时发送。

### 上传与存储方案
1. **云存储选择**
   - *iCloud Drive / Google Drive / Dropbox*：手机自动备份相册，跨平台支持好，但访问 API 需配额。
   - *阿里云 OSS / 腾讯 COS / AWS S3*：可配合三方 App 或小程序上传，适合自建管控。
   - *NAS（Synology/QNAP）*：手机通过官方 App 直接同步到自家 NAS，再由 NAS 提供 WebDAV/S3 协议给下游读取。
2. **目录规范**
   ```
   /media-daily/
     YYYY/
       MM/
         DD/
           original/
             IMG_20241101_123456.jpg
             VID_20241101_123500.mp4
           metadata.json
   ```
   - `metadata.json` 记录每个素材的标题、描述、拍摄地点等信息（可由手机端或后端补齐）。
3. **同步工具**
   - iOS：利用快捷指令 + 云盘 App（如 Dropbox）实现“一键上传”或后台自动同步。
   - Android：使用 Tasker/自动上传 App（OneDrive/Google Photos/阿里云盘）。
   - NAS：开启手机自动备份（Moments / Photo Station）。

### AI 处理流程
1. **事件触发**
   - 使用云端定时任务（GitHub Actions / 云函数）每日扫描新目录。
   - 从 `metadata.json` 或对象存储的文件属性中读取新增条目。
2. **数据下载/预处理**
   - 图片：生成缩略图（降低分辨率），使用 OCR/图像描述模型提取内容。
   - 视频：截取关键帧或调用短视频摘要模型（需 GPU 或 LLM API）。
3. **大模型调用**
   - 选择支持图像/视频输入的模型（如 OpenAI GPT-4o、腾讯混元、阿里通义等），结合所在地法规与网络环境挑选供应商。
   - 输入包括：素材路径（或 CDN 链接）、提取的标签/位置、可选的手工注释。
   - 输出：标题、摘要、故事性文案、行动号召等。
   - 读取素材的方式：
     - 若素材存放在云存储（OSS/COS/S3），根据 `metadata.json` 生成公开或限时签名 URL，直接传给模型。
     - 模型支持文件上传时，可在运行环境将媒体临时下载后随 API 请求上传，避免暴露链接。
     - 脚本需维护一个索引（例如 `metadata.json` 内的 `processed: true/false`），确保每个文件只处理一次。

### 模型调用部署方式
- **GitHub Actions（无服务器）**
  - 在现有工作流中插入 `python scripts/generate_media_copy.py` 步骤，利用 `actions/setup-python` 安装依赖，使用 Secrets 注入 `OPENAI_API_KEY`、`QWEN_API_KEY` 等凭据。
  - 适合调用境外公共 API。执行结束后把生成的 Markdown 追加到仓库或创建 PR。
  - 注意 GitHub Actions 单次运行时长（6 小时）、网络连通性（部分国内模型可能不可达）以及 API 费用。
- **自建服务器 / NAS / 自托管 Runner**
  - 在云主机、NAS 或自托管 runner 上运行相同脚本，通过 cron 定时触发。
  - 优点：可直接访问本地存储、使用国内模型、执行时间不受限制；缺点：需自行维护环境安全与更新。
  - 可与 GitHub Actions 结合——在服务器上部署 self-hosted runner，由 Actions 触发但在本地执行。

### 与知识库及文件系统的整合
- **Obsidian / Markdown Vault**
  - 将 `media-daily` 或 `daily-media-summaries.md` 直接放入 Obsidian Vault，使用 YAML front matter 存储元数据，配合 Dataview 展示图库与文案。
  - 生成脚本可以读取这些 Markdown 文件，将其视为知识库：每条记录的 front matter 提供标签、拍摄时间、媒体链接等结构化信息。
- **知识库 + 自动化**
  - 如需全文搜索或问答，可将摘要同步到向量数据库（Pinecone/Milvus）或现成的知识库服务，再由大模型结合上下文生成邮件内容。

### 媒体文件存放位置讨论
- **不建议直接推到 GitHub 仓库**
  - 图片/视频体积大，频繁更新会导致仓库膨胀；GitHub 对单文件和历史大小有限制，超过 100MB 还需 LFS。
  - 隐私方面：公开仓库会暴露素材，即便私有仓库也存在误分享风险。
- **推荐策略**
  - 将原始媒体存储在云存储或 NAS，对外暴露经过权限控制的 URL；Git 仓库中仅保留引用信息（路径、签名链接、元数据）。
  - 如确需版本控制，可在 Git 中保存压缩后的缩略图或以 Git LFS 管理，并定期清理。
4. **结果存储**
   - 将生成的内容写入新的 Markdown（如 `daily-media-summaries.md`），结构类似：
     ```
     ## 2024-11-02｜IMG_20241102_101010.jpg
     - 拍摄时间：2024-11-02 10:10
     - 位置：上海·外滩
     - 模型解读：……
     - 推荐配文：……
     ```
   - 同步保存关联的原图链接或缩略图 URL。

### 邮件集成
- GitHub Actions 每日调度：
  1. 拉取最新媒体摘要 Markdown。
  2. 调用现有 `send_pyq_snippet.py` 或新的脚本，将当天新增的一个/多个段落发送给订阅者。
- 若需要附件，可在邮件中嵌入媒体 URL（注意访问权限）或使用 CID 嵌图。

### 权限与安全
1. 云存储需设定访问凭据，通过 GitHub Secrets 或服务器的环境变量保存。
2. AI 服务的 API Key 也需加密存储，谨防泄露。
3. 邮件收件人隐私保护，遵循订阅取消机制。

### 后续扩展
- 为素材自动打标签（位置、主题、情绪、人物），便于检索。
- 制作 Web 仪表盘展示每日生成的媒体+文案、统计发送情况。
- 在手机端加一个提示流程，让用户选哪些素材需要对外发送。
- 引入 A/B 测试：针对同一素材生成多条文案，挑选互动更好的版本。
